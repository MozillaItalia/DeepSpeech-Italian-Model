{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deepspeech_lm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXld1lBFASOQ",
        "colab_type": "text"
      },
      "source": [
        "<H1> Language model</H1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKHFpgPeTKhe",
        "colab_type": "code",
        "outputId": "b2e47310-9802-497c-f9d3-bcc20db1e7fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# /root will be the main dir\n",
        "H = %env HOME\n",
        "%cd $H"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0lqFnePVmoZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!apt update"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkbyLhqqSONM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#kenlm dependencies\n",
        "!sudo apt install build-essential cmake libboost-system-dev libboost-thread-dev libboost-program-options-dev libboost-test-dev libeigen3-dev zlib1g-dev libbz2-dev liblzma-dev\n",
        "#kenlm\n",
        "!git clone https://github.com/kpu/kenlm.git && cd kenlm && mkdir -p build\n",
        "%cd kenlm/build\n",
        "!cmake .. && make -j 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsFBffBkUUtk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cd $H\n",
        "!git clone https://github.com/mozilla/DeepSpeech.git\n",
        "!git clone https://github.com/MozillaItalia/DeepSpeech-Italian-Model.git"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mD3AkqYVc8S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell if you need the wiki_it_lower.txt file\n",
        "!apt install pixz\n",
        "!set -xe\n",
        "# TODO tr upper to lower is useless?\n",
        "!curl -sSL https://github.com/MozillaItalia/DeepSpeech-Italian-Model/releases/download/lm-0.1/wiki.txt.xz | pixz -d > wiki_it.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q5CxTSCpUw7w",
        "colab_type": "code",
        "outputId": "bcc2aab4-4fb5-4ca8-95a4-b40952838616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd DeepSpeech/data/lm\n",
        "!set -xe\n",
        "!python generate_lm.py --input_txt \"$H/wiki_it.txt\" --output_dir \"$H\" \\\n",
        "  --top_k 500000 --kenlm_bins \"/root/kenlm/build/bin\" \\\n",
        "  --arpa_order 5 --max_arpa_memory \"75%\" --arpa_prune \"0|0|1\" \\\n",
        "  --binary_a_bits 255 --binary_q_bits 8 --binary_type trie"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'DeepSpeech/data/lm'\n",
            "/root/DeepSpeech/data/lm\n",
            "\n",
            "Converting to lowercase and counting word occurrences ...\n",
            "| |     #                                       | 21249410 Elapsed Time: 0:09:35\n",
            "\n",
            "Saving top 500000 words ...\n",
            "\n",
            "Calculating word statistics ...\n",
            "  Your text file has 427255084 words in total\n",
            "  It has 7360290 unique words\n",
            "  Your top-500000 words are 96.2131 percent of all words\n",
            "  Your most common word \"di\" occurred 17423440 times\n",
            "  The least common word in your top-k is \"(8â€“6)\" with 19 times\n",
            "  The first word with 20 occurrences is \"gowadia\" at place 482206\n",
            "\n",
            "Creating ARPA file ...\n",
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /root/lower.txt.gz\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "tcmalloc: large alloc 2043592704 bytes == 0x556378b04000 @  0x7f08a7ec21e7 0x5563777f9772 0x55637778d358 0x55637776c290 0x556377758096 0x7f08a605bb97 0x556377759ada\n",
            "tcmalloc: large alloc 9536757760 bytes == 0x5563f27f0000 @  0x7f08a7ec21e7 0x5563777f9772 0x5563777e37aa 0x5563777e41c8 0x55637776c2ad 0x556377758096 0x7f08a605bb97 0x556377759ada\n",
            "****************************************************************************************************\n",
            "Unigram tokens 426890657 types 7471683\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:89660196 2:1103856128 3:2069730432 4:3311568640 5:4829370880\n",
            "tcmalloc: large alloc 4829372416 bytes == 0x5563789f6000 @  0x7f08a7ec21e7 0x5563777f9772 0x5563777e37aa 0x5563777e41c8 0x55637776c84e 0x556377758096 0x7f08a605bb97 0x556377759ada\n",
            "tcmalloc: large alloc 2069733376 bytes == 0x5564df9e0000 @  0x7f08a7ec21e7 0x5563777f9772 0x5563777e37aa 0x5563777e41c8 0x55637776cc3d 0x556377758096 0x7f08a605bb97 0x556377759ada\n",
            "tcmalloc: large alloc 3311575040 bytes == 0x55662b77a000 @  0x7f08a7ec21e7 0x5563777f9772 0x5563777e37aa 0x5563777e41c8 0x55637776cc3d 0x556377758096 0x7f08a605bb97 0x556377759ada\n",
            "Statistics:\n",
            "1 7471683 D1=0.721746 D2=1.04424 D3+=1.32649\n",
            "2 69061903 D1=0.793529 D2=1.09422 D3+=1.31473\n",
            "3 33887666/190669371 D1=0.860851 D2=1.15161 D3+=1.33121\n",
            "4 30103458/289590000 D1=0.920405 D2=1.23316 D3+=1.3652\n",
            "5 20040199/331015902 D1=0.92132 D2=1.37928 D3+=1.44798\n",
            "Memory estimate for binary LM:\n",
            "type      MB\n",
            "probing 3574 assuming -p 1.5\n",
            "probing 4364 assuming -r models -p 1.5\n",
            "trie    2068 without quantization\n",
            "trie    1268 assuming -q 8 -b 8 quantization \n",
            "trie    1763 assuming -a 22 array pointer compression\n",
            "trie     962 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "tcmalloc: large alloc 3813392384 bytes == 0x5563789f6000 @  0x7f08a7ec21e7 0x5563777f9772 0x556377777232 0x5563777781cd 0x7f08a77f5bcd 0x7f08a75cc6db 0x7f08a615b88f\n",
            "tcmalloc: large alloc 6950166528 bytes == 0x55662b77a000 @  0x7f08a7ec21e7 0x5563777f9772 0x556377777232 0x5563777781cd 0x7f08a77f5bcd 0x7f08a75cc6db 0x7f08a615b88f\n",
            "tcmalloc: large alloc 9268453376 bytes == 0x55662b77a000 @  0x7f08a7ec21e7 0x5563777f9772 0x556377777232 0x5563777781cd 0x7f08a77f5bcd 0x7f08a75cc6db 0x7f08a615b88f\n",
            "Chain sizes: 1:89660196 2:1104990448 3:677753320 4:722482992 5:561125572\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "******##############################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:89660196 2:1104990448 3:677753320 4:722482992 5:561125572\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:21789600 kB\tVmRSS:3164804 kB\tRSSMax:11426192 kB\tuser:893.99\tsys:149.716\tCPU:1043.71\treal:2273.46\n",
            "\n",
            "Filtering ARPA file using vocabulary of top-k words ...\n",
            "Reading /root/lm.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "\n",
            "Building lm.binary ...\n",
            "Reading /root/lm_filtered.arpa\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Identifying n-grams omitted by SRI\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Quantizing\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Writing trie\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-rEBL2Qrj5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#before run generate_package install all deepspeech dependencies\n",
        "%cd $H/DeepSpeech\n",
        "%tensorflow_version 1.x\n",
        "!pip3 install --no-cache-dir --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
        "!pip3 install --no-cache-dir --upgrade -e ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i6R19ir8Xlx",
        "colab_type": "code",
        "outputId": "698fecd0-bfa8-43ad-8da4-c1515d259c56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "%cd $H/DeepSpeech/data/lm\n",
        "!python generate_package.py --alphabet $H/DeepSpeech-Italian-Model/DeepSpeech/italian_alphabet.txt \\\n",
        "  --lm \"$H/lm.binary\" \\\n",
        "  --vocab \"$H/vocab-500000.txt\" \\\n",
        "  --package \"$H/kenlm.scorer\" \\\n",
        "  --default_alpha 0.931289039105002 \\\n",
        "  --default_beta 1.1834137581510284\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root/DeepSpeech/data/lm\n",
            "500000 unique words read from vocabulary file.\n",
            "Doesn't look like a character based model.\n",
            "Using detected UTF-8 mode: False\n",
            "Package created in /root/kenlm.scorer\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}