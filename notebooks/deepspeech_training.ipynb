{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yBVpKaIDxr5n"
   },
   "source": [
    "# <h1> A COLAB notebook for italian DeepSpeech model </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E-6vTdO315En"
   },
   "source": [
    "## Install all the needed dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nf0QISLC2nes",
    "outputId": "22c008b4-593c-4330-d562-ac03695da176"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n"
     ]
    }
   ],
   "source": [
    "# shortcut for /root folder\n",
    "H = %env HOME\n",
    "%cd $H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "id": "qK-aUbVj8Eey",
    "outputId": "d8aaf7ab-3069-4d58-e4e4-f17d25bfbf06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root\n",
      "Cloning into './ds'...\n",
      "remote: Enumerating objects: 18215, done.\u001b[K\n",
      "remote: Total 18215 (delta 0), reused 0 (delta 0), pack-reused 18215\u001b[K\n",
      "Receiving objects: 100% (18215/18215), 47.57 MiB | 12.40 MiB/s, done.\n",
      "Resolving deltas: 100% (12392/12392), done.\n",
      "/root/ds\n",
      "Note: checking out '2e9c281d06ea8da97f7e4eebd3e4476350e7776a'.\n",
      "\n",
      "You are in 'detached HEAD' state. You can look around, make experimental\n",
      "changes and commit them, and you can discard any commits you make in this\n",
      "state without impacting any branches by performing another checkout.\n",
      "\n",
      "If you want to create a new branch to retain commits you create, you may\n",
      "do so (now or later) by using -b with the checkout command again. Example:\n",
      "\n",
      "  git checkout -b <new-branch-name>\n",
      "\n",
      "HEAD is now at 2e9c281d Merge pull request #2990 from mozilla/release-071\n"
     ]
    }
   ],
   "source": [
    "# Get DeepSpeech repo pointing to 0.7.1 release\n",
    "\n",
    "%cd $H\n",
    "# !apt-get install git-lfs\n",
    "!git clone https://github.com/mozilla/DeepSpeech.git ./ds\n",
    "%cd ds\n",
    "!git checkout 2e9c281d06ea8da97f7e4eebd3e4476350e7776a\n",
    "# !git-lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "5k0Pegar2_7w",
    "outputId": "2cf85ab3-a527-4899-e36e-11dd43cbbf91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 1.x selected.\n",
      "Collecting pip==20.0.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/0c/d01aa759fdc501a58f431eb594a17495f15b88da142ce14b5845662c13f3/pip-20.0.2-py2.py3-none-any.whl (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 2.7MB/s \n",
      "\u001b[?25hRequirement already up-to-date: wheel==0.34.2 in /usr/local/lib/python3.6/dist-packages (0.34.2)\n",
      "Collecting setuptools==46.1.3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/df/635cdb901ee4a8a42ec68e480c49f85f4c59e8816effbf57d9e6ee8b3588/setuptools-46.1.3-py3-none-any.whl (582kB)\n",
      "\u001b[K     |████████████████████████████████| 583kB 17.9MB/s \n",
      "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pip, setuptools\n",
      "  Found existing installation: pip 19.3.1\n",
      "    Uninstalling pip-19.3.1:\n",
      "      Successfully uninstalled pip-19.3.1\n",
      "  Found existing installation: setuptools 46.3.0\n",
      "    Uninstalling setuptools-46.3.0:\n",
      "      Successfully uninstalled setuptools-46.3.0\n",
      "Successfully installed pip-20.0.2 setuptools-46.1.3\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "pkg_resources"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///root/ds\n",
      "Requirement already satisfied, skipping upgrade: tensorflow==1.15.2 in /tensorflow-1.15.2/python3.6 (from deepspeech-training==0.7.1) (1.15.2)\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (1.18.4)\n",
      "Requirement already satisfied, skipping upgrade: progressbar2 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (3.38.0)\n",
      "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (1.12.0)\n",
      "Collecting pyxdg\n",
      "  Downloading pyxdg-0.26-py2.py3-none-any.whl (40 kB)\n",
      "\u001b[K     |████████████████████████████████| 40 kB 2.1 MB/s \n",
      "\u001b[?25hCollecting attrdict\n",
      "  Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
      "Requirement already satisfied, skipping upgrade: absl-py in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (0.9.0)\n",
      "Collecting semver\n",
      "  Downloading semver-2.10.1-py2.py3-none-any.whl (12 kB)\n",
      "Collecting opuslib==2.0.0\n",
      "  Downloading opuslib-2.0.0.tar.gz (7.3 kB)\n",
      "Collecting optuna\n",
      "  Downloading optuna-1.4.0.tar.gz (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 4.0 MB/s \n",
      "\u001b[?25hCollecting sox\n",
      "  Downloading sox-1.3.7-py2.py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied, skipping upgrade: bs4 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (0.0.1)\n",
      "Requirement already satisfied, skipping upgrade: pandas in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (1.0.3)\n",
      "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (2.23.0)\n",
      "Collecting numba==0.47.0\n",
      "  Downloading numba-0.47.0-cp36-cp36m-manylinux1_x86_64.whl (3.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.7 MB 4.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: llvmlite==0.31.0 in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (0.31.0)\n",
      "Requirement already satisfied, skipping upgrade: librosa in /usr/local/lib/python3.6/dist-packages (from deepspeech-training==0.7.1) (0.6.3)\n",
      "Collecting soundfile\n",
      "  Downloading SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
      "Collecting ds_ctcdecoder@ https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.7.1.cpu-ctc/artifacts/public/ds_ctcdecoder-0.7.1-cp36-cp36m-manylinux1_x86_64.whl\n",
      "  Downloading https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.deepspeech.native_client.v0.7.1.cpu-ctc/artifacts/public/ds_ctcdecoder-0.7.1-cp36-cp36m-manylinux1_x86_64.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 1.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (0.34.2)\n",
      "Requirement already satisfied, skipping upgrade: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (3.10.0)\n",
      "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (0.2.0)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
      "Requirement already satisfied, skipping upgrade: tensorflow-estimator==1.15.1 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15.2->deepspeech-training==0.7.1) (1.15.1)\n",
      "Requirement already satisfied, skipping upgrade: tensorboard<1.16.0,>=1.15.0 in /tensorflow-1.15.2/python3.6 (from tensorflow==1.15.2->deepspeech-training==0.7.1) (1.15.0)\n",
      "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (1.12.1)\n",
      "Requirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (1.0.8)\n",
      "Requirement already satisfied, skipping upgrade: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (0.8.1)\n",
      "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (1.28.1)\n",
      "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2->deepspeech-training==0.7.1) (1.1.0)\n",
      "Requirement already satisfied, skipping upgrade: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->deepspeech-training==0.7.1) (2.4.0)\n",
      "Collecting alembic\n",
      "  Downloading alembic-1.4.2.tar.gz (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 26.5 MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting cliff\n",
      "  Downloading cliff-3.1.0-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 48.6 MB/s \n",
      "\u001b[?25hCollecting cmaes>=0.3.2\n",
      "  Downloading cmaes-0.5.0-py3-none-any.whl (13 kB)\n",
      "Collecting colorlog\n",
      "  Downloading colorlog-4.1.0-py2.py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.7.1) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy!=1.4.0 in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.7.1) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: sqlalchemy>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.7.1) (1.3.16)\n",
      "Requirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.6/dist-packages (from optuna->deepspeech-training==0.7.1) (4.41.1)\n",
      "Requirement already satisfied, skipping upgrade: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from bs4->deepspeech-training==0.7.1) (4.6.3)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->deepspeech-training==0.7.1) (2.8.1)\n",
      "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->deepspeech-training==0.7.1) (2018.9)\n",
      "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.7.1) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.7.1) (2020.4.5.1)\n",
      "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.7.1) (1.24.3)\n",
      "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->deepspeech-training==0.7.1) (2.9)\n",
      "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from numba==0.47.0->deepspeech-training==0.7.1) (46.1.3)\n",
      "Requirement already satisfied, skipping upgrade: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.7.1) (4.4.2)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.7.1) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.7.1) (2.1.8)\n",
      "Requirement already satisfied, skipping upgrade: resampy>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from librosa->deepspeech-training==0.7.1) (0.2.2)\n",
      "Requirement already satisfied, skipping upgrade: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile->deepspeech-training==0.7.1) (1.14.0)\n",
      "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->deepspeech-training==0.7.1) (3.2.1)\n",
      "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2->deepspeech-training==0.7.1) (1.0.1)\n",
      "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2->deepspeech-training==0.7.1) (2.10.0)\n",
      "Collecting python-editor>=0.3\n",
      "  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.1.2-py2.py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 54.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: pyparsing>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->deepspeech-training==0.7.1) (2.4.7)\n",
      "Requirement already satisfied, skipping upgrade: PyYAML>=3.12 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->deepspeech-training==0.7.1) (3.13)\n",
      "Collecting stevedore>=1.20.0\n",
      "  Downloading stevedore-1.32.0-py2.py3-none-any.whl (43 kB)\n",
      "\u001b[K     |████████████████████████████████| 43 kB 48.5 MB/s \n",
      "\u001b[?25hCollecting cmd2!=0.8.3,<0.9.0,>=0.8.0\n",
      "  Downloading cmd2-0.8.9-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 42.3 MB/s \n",
      "\u001b[?25hCollecting pbr!=2.1.0,>=2.0.0\n",
      "  Downloading pbr-5.4.5-py2.py3-none-any.whl (110 kB)\n",
      "\u001b[K     |████████████████████████████████| 110 kB 31.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: PrettyTable<0.8,>=0.7.2 in /usr/local/lib/python3.6/dist-packages (from cliff->optuna->deepspeech-training==0.7.1) (0.7.2)\n",
      "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile->deepspeech-training==0.7.1) (2.20)\n",
      "Requirement already satisfied, skipping upgrade: MarkupSafe>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from Mako->alembic->optuna->deepspeech-training==0.7.1) (1.1.1)\n",
      "Requirement already satisfied, skipping upgrade: wcwidth; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from cmd2!=0.8.3,<0.9.0,>=0.8.0->cliff->optuna->deepspeech-training==0.7.1) (0.1.9)\n",
      "Collecting pyperclip\n",
      "  Downloading pyperclip-1.8.0.tar.gz (16 kB)\n",
      "Building wheels for collected packages: opuslib, optuna, gast, alembic, pyperclip\n",
      "  Building wheel for opuslib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for opuslib: filename=opuslib-2.0.0-py3-none-any.whl size=11009 sha256=fda370c53acac5a07dbfd645173cbafd00d15f31f31de973da975dfa877119cd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1fhdp__z/wheels/6c/01/88/37797e9e9d157a33eefed22a46aa0bf5044effcec6a9181e41\n",
      "  Building wheel for optuna (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for optuna: filename=optuna-1.4.0-py3-none-any.whl size=254554 sha256=f9d4f0b54e6ff85580eb5085311e4632c2464f118dbd54d18d7a69e6b83c19a6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1fhdp__z/wheels/d9/41/ee/b6222654d95de1c293b3dbc83a1c4c9335d2314765a9292c4e\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7539 sha256=4c794c6577a754294a400fb735ed7de562284c3e98b05bf42175ba8465e52e93\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1fhdp__z/wheels/19/a7/b9/0740c7a3a7d1d348f04823339274b90de25fbcd217b2ee1fbe\n",
      "  Building wheel for alembic (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for alembic: filename=alembic-1.4.2-py2.py3-none-any.whl size=159543 sha256=16e88e2ce96db304f560503db91705ba212e610785537458eb66d30a6deedeca\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1fhdp__z/wheels/f2/50/61/5cc491b0ca39be60dfb4dce940b389ff91b847d62e0eb2d680\n",
      "  Building wheel for pyperclip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyperclip: filename=pyperclip-1.8.0-py3-none-any.whl size=8691 sha256=a96214fb5ac50c23609ace4a4c1ba0cebf2db0f30f39ff109a8aa1e8bb5dddfe\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-1fhdp__z/wheels/26/30/fe/92e2d4b1301ba74c07ea09c9e4c08f5bf12bae9c30319d74c5\n",
      "Successfully built opuslib optuna gast alembic pyperclip\n",
      "\u001b[31mERROR: umap-learn 0.4.2 has requirement numba!=0.47,>=0.46, but you'll have numba 0.47.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: pyxdg, attrdict, semver, opuslib, python-editor, Mako, alembic, pbr, stevedore, pyperclip, cmd2, cliff, cmaes, colorlog, optuna, sox, numba, soundfile, ds-ctcdecoder, deepspeech-training, gast\n",
      "  Attempting uninstall: numba\n",
      "    Found existing installation: numba 0.48.0\n",
      "    Uninstalling numba-0.48.0:\n",
      "      Successfully uninstalled numba-0.48.0\n",
      "  Running setup.py develop for deepspeech-training\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "Successfully installed Mako-1.1.2 alembic-1.4.2 attrdict-2.0.1 cliff-3.1.0 cmaes-0.5.0 cmd2-0.8.9 colorlog-4.1.0 deepspeech-training ds-ctcdecoder-0.7.1 gast-0.2.2 numba-0.47.0 optuna-1.4.0 opuslib-2.0.0 pbr-5.4.5 pyperclip-1.8.0 python-editor-1.0.4 pyxdg-0.26 semver-2.10.1 soundfile-0.10.3.post1 sox-1.3.7 stevedore-1.32.0\n",
      "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
      "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
      "Hit:3 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
      "Get:6 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
      "Get:7 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [912 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [846 kB]\n",
      "Get:10 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [92.1 kB]\n",
      "Ign:11 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
      "Ign:12 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
      "Hit:13 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
      "Get:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,816 kB]\n",
      "Hit:15 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
      "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,376 kB]\n",
      "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,207 kB]\n",
      "Get:20 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [876 kB]\n",
      "Fetched 7,396 kB in 3s (2,701 kB/s)\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "42 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libid3tag0 libmad0 libmagic-mgc libmagic1 libopencore-amrnb0\n",
      "  libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox3\n",
      "Suggested packages:\n",
      "  file libsox-fmt-all\n",
      "The following NEW packages will be installed:\n",
      "  libid3tag0 libmad0 libmagic-mgc libmagic1 libopencore-amrnb0\n",
      "  libopencore-amrwb0 libsox-fmt-alsa libsox-fmt-base libsox-fmt-mp3 libsox3\n",
      "  pixz sox\n",
      "0 upgraded, 12 newly installed, 0 to remove and 42 not upgraded.\n",
      "Need to get 892 kB of archives.\n",
      "After this operation, 7,143 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrnb0 amd64 0.1.3-2.1 [92.0 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libopencore-amrwb0 amd64 0.1.3-2.1 [45.8 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic-mgc amd64 1:5.32-2ubuntu0.4 [184 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libmagic1 amd64 1:5.32-2ubuntu0.4 [68.6 kB]\n",
      "Get:5 http://archive.ubuntu.com/ubuntu bionic/universe amd64 libid3tag0 amd64 0.15.1b-13 [31.2 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libmad0 amd64 0.15.1b-9ubuntu18.04.1 [64.6 kB]\n",
      "Get:7 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox3 amd64 14.4.2-3ubuntu0.18.04.1 [226 kB]\n",
      "Get:8 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-alsa amd64 14.4.2-3ubuntu0.18.04.1 [10.6 kB]\n",
      "Get:9 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-base amd64 14.4.2-3ubuntu0.18.04.1 [32.1 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 libsox-fmt-mp3 amd64 14.4.2-3ubuntu0.18.04.1 [15.9 kB]\n",
      "Get:11 http://archive.ubuntu.com/ubuntu bionic/universe amd64 pixz amd64 1.0.6-2build1 [20.8 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 sox amd64 14.4.2-3ubuntu0.18.04.1 [101 kB]\n",
      "Fetched 892 kB in 1s (1,089 kB/s)\n",
      "Selecting previously unselected package libopencore-amrnb0:amd64.\n",
      "(Reading database ... 144433 files and directories currently installed.)\n",
      "Preparing to unpack .../00-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\n",
      "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
      "Selecting previously unselected package libopencore-amrwb0:amd64.\n",
      "Preparing to unpack .../01-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\n",
      "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
      "Selecting previously unselected package libmagic-mgc.\n",
      "Preparing to unpack .../02-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
      "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
      "Selecting previously unselected package libmagic1:amd64.\n",
      "Preparing to unpack .../03-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\n",
      "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
      "Selecting previously unselected package libid3tag0:amd64.\n",
      "Preparing to unpack .../04-libid3tag0_0.15.1b-13_amd64.deb ...\n",
      "Unpacking libid3tag0:amd64 (0.15.1b-13) ...\n",
      "Selecting previously unselected package libmad0:amd64.\n",
      "Preparing to unpack .../05-libmad0_0.15.1b-9ubuntu18.04.1_amd64.deb ...\n",
      "Unpacking libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
      "Selecting previously unselected package libsox3:amd64.\n",
      "Preparing to unpack .../06-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package libsox-fmt-alsa:amd64.\n",
      "Preparing to unpack .../07-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package libsox-fmt-base:amd64.\n",
      "Preparing to unpack .../08-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package libsox-fmt-mp3:amd64.\n",
      "Preparing to unpack .../09-libsox-fmt-mp3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Selecting previously unselected package pixz.\n",
      "Preparing to unpack .../10-pixz_1.0.6-2build1_amd64.deb ...\n",
      "Unpacking pixz (1.0.6-2build1) ...\n",
      "Selecting previously unselected package sox.\n",
      "Preparing to unpack .../11-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\n",
      "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Setting up libid3tag0:amd64 (0.15.1b-13) ...\n",
      "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\n",
      "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\n",
      "Setting up pixz (1.0.6-2build1) ...\n",
      "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\n",
      "Setting up libmad0:amd64 (0.15.1b-9ubuntu18.04.1) ...\n",
      "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\n",
      "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Setting up libsox-fmt-mp3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
      "\n",
      "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
      "Processing triggers for mime-support (3.60ubuntu1) ...\n"
     ]
    }
   ],
   "source": [
    "# Tell colab to use TF 1.x version and then install DS dependencies\n",
    "%tensorflow_version 1.x\n",
    "!pip3 install --no-cache-dir --upgrade pip==20.0.2 wheel==0.34.2 setuptools==46.1.3\n",
    "!pip3 install --no-cache-dir --upgrade -e .\n",
    "!apt update\n",
    "!apt-get install sox libsox-fmt-mp3 pixz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "LPiM0nZ88Yfm",
    "outputId": "3f106dd3-62f6-4b4a-fd7a-d42fb4efccf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ ldc93s1_dir=./data/smoke_test\n",
      "+ ldc93s1_csv=./data/smoke_test/ldc93s1.csv\n",
      "+ epoch_count=2\n",
      "+ audio_sample_rate=16000\n",
      "+ [ ! -f ./data/smoke_test/ldc93s1.csv ]\n",
      "+ echo Downloading and preprocessing LDC93S1 example data, saving in ./data/smoke_test.\n",
      "Downloading and preprocessing LDC93S1 example data, saving in ./data/smoke_test.\n",
      "+ python -u bin/import_ldc93s1.py ./data/smoke_test\n",
      "Found archive \"./data/smoke_test/LDC93S1.wav\" - not downloading.\n",
      "Found archive \"./data/smoke_test/LDC93S1.txt\" - not downloading.\n",
      "+ export CUDA_VISIBLE_DEVICES=0\n",
      "+ python -u DeepSpeech.py --noshow_progressbar --noearly_stop --train_files ./data/smoke_test/ldc93s1.csv --train_batch_size 1 --feature_cache /tmp/ldc93s1_cache --dev_files ./data/smoke_test/ldc93s1.csv --dev_batch_size 1 --test_files ./data/smoke_test/ldc93s1.csv --test_batch_size 1 --n_hidden 100 --epochs 2 --max_to_keep 1 --checkpoint_dir /tmp/ckpt --learning_rate 0.001 --dropout_rate 0.05 --export_dir /tmp/train --scorer_path data/smoke_test/pruned_lm.scorer --audio_sample_rate 16000\n",
      "I Could not find best validating checkpoint.\n",
      "I Could not find most recent checkpoint.\n",
      "I Initializing all variables.\n",
      "I STARTING Optimization\n",
      "I Training epoch 0...\n",
      "I Finished training epoch 0 - loss: 342.797699\n",
      "I Validating epoch 0 on ./data/smoke_test/ldc93s1.csv...\n",
      "I Finished validating epoch 0 on ./data/smoke_test/ldc93s1.csv - loss: 294.708832\n",
      "I Saved new best validating model with loss 294.708832 to: /tmp/ckpt/best_dev-1\n",
      "I Training epoch 1...\n",
      "I Finished training epoch 1 - loss: 298.943695\n",
      "I Validating epoch 1 on ./data/smoke_test/ldc93s1.csv...\n",
      "I Finished validating epoch 1 on ./data/smoke_test/ldc93s1.csv - loss: 258.742615\n",
      "I Saved new best validating model with loss 258.742615 to: /tmp/ckpt/best_dev-2\n",
      "I FINISHED optimization in 0:00:07.540399\n",
      "I Loading best validating checkpoint from /tmp/ckpt/best_dev-2\n",
      "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
      "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
      "I Loading variable from checkpoint: global_step\n",
      "I Loading variable from checkpoint: layer_1/bias\n",
      "I Loading variable from checkpoint: layer_1/weights\n",
      "I Loading variable from checkpoint: layer_2/bias\n",
      "I Loading variable from checkpoint: layer_2/weights\n",
      "I Loading variable from checkpoint: layer_3/bias\n",
      "I Loading variable from checkpoint: layer_3/weights\n",
      "I Loading variable from checkpoint: layer_5/bias\n",
      "I Loading variable from checkpoint: layer_5/weights\n",
      "I Loading variable from checkpoint: layer_6/bias\n",
      "I Loading variable from checkpoint: layer_6/weights\n",
      "Testing model on ./data/smoke_test/ldc93s1.csv\n",
      "I Test epoch...\n",
      "Test on ./data/smoke_test/ldc93s1.csv - WER: 1.000000, CER: 0.826923, loss: 258.742615\n",
      "--------------------------------------------------------------------------------\n",
      "Best WER: \n",
      "--------------------------------------------------------------------------------\n",
      "WER: 1.000000, CER: 0.826923, loss: 258.742615\n",
      " - wav: file:///root/ds/data/smoke_test/LDC93S1.wav\n",
      " - src: \"she had your dark suit in greasy wash water all year\"\n",
      " - res: \"notwithstanding he took\"\n",
      "--------------------------------------------------------------------------------\n",
      "Median WER: \n",
      "--------------------------------------------------------------------------------\n",
      "WER: 1.000000, CER: 0.826923, loss: 258.742615\n",
      " - wav: file:///root/ds/data/smoke_test/LDC93S1.wav\n",
      " - src: \"she had your dark suit in greasy wash water all year\"\n",
      " - res: \"notwithstanding he took\"\n",
      "--------------------------------------------------------------------------------\n",
      "Worst WER: \n",
      "--------------------------------------------------------------------------------\n",
      "WER: 1.000000, CER: 0.826923, loss: 258.742615\n",
      " - wav: file:///root/ds/data/smoke_test/LDC93S1.wav\n",
      " - src: \"she had your dark suit in greasy wash water all year\"\n",
      " - res: \"notwithstanding he took\"\n",
      "--------------------------------------------------------------------------------\n",
      "I Exporting the model...\n",
      "I Loading best validating checkpoint from /tmp/ckpt/best_dev-2\n",
      "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/bias\n",
      "I Loading variable from checkpoint: cudnn_lstm/rnn/multi_rnn_cell/cell_0/cudnn_compatible_lstm_cell/kernel\n",
      "I Loading variable from checkpoint: layer_1/bias\n",
      "I Loading variable from checkpoint: layer_1/weights\n",
      "I Loading variable from checkpoint: layer_2/bias\n",
      "I Loading variable from checkpoint: layer_2/weights\n",
      "I Loading variable from checkpoint: layer_3/bias\n",
      "I Loading variable from checkpoint: layer_3/weights\n",
      "I Loading variable from checkpoint: layer_5/bias\n",
      "I Loading variable from checkpoint: layer_5/weights\n",
      "I Loading variable from checkpoint: layer_6/bias\n",
      "I Loading variable from checkpoint: layer_6/weights\n",
      "I Models exported at /tmp/train\n",
      "I Model metadata file saved to /tmp/train/author_model_0.0.1.md. Before submitting the exported model for publishing make sure all information in the metadata file is correct, and complete the URL fields.\n"
     ]
    }
   ],
   "source": [
    "# simple check before going on\n",
    "!./bin/run-tc-ldc93s1_new.sh 2 16000\n",
    "\n",
    "# this one need the kenlm.scorer so you need to install git-lfs and do a \n",
    "# git-fls pull in the deepspeech repo\n",
    "# !./bin/run-ldc93s1.sh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "33pDRwMPO214",
    "outputId": "4ba7da30-be6f-44b6-c02c-74d175b53142"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CV_TINY_PATH=/mnt/extracted/data/cv-it_tiny\n",
      "--2020-05-18 12:58:30--  https://github.com/MozillaItalia/DeepSpeech-Italian-Model/files/4610711/cv-it_tiny.tar.gz\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-repository-file-5c1aeb.s3.amazonaws.com/189566628/4610711?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200518%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200518T125831Z&X-Amz-Expires=300&X-Amz-Signature=b1adf63c578a3f69e1956663d1208ab8dae33b5b888a615857cae0941c75e8dd&X-Amz-SignedHeaders=host&actor_id=0&repo_id=189566628&response-content-disposition=attachment%3Bfilename%3Dcv-it_tiny.tar.gz&response-content-type=application%2Fgzip [following]\n",
      "--2020-05-18 12:58:31--  https://github-production-repository-file-5c1aeb.s3.amazonaws.com/189566628/4610711?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200518%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200518T125831Z&X-Amz-Expires=300&X-Amz-Signature=b1adf63c578a3f69e1956663d1208ab8dae33b5b888a615857cae0941c75e8dd&X-Amz-SignedHeaders=host&actor_id=0&repo_id=189566628&response-content-disposition=attachment%3Bfilename%3Dcv-it_tiny.tar.gz&response-content-type=application%2Fgzip\n",
      "Resolving github-production-repository-file-5c1aeb.s3.amazonaws.com (github-production-repository-file-5c1aeb.s3.amazonaws.com)... 52.216.100.187\n",
      "Connecting to github-production-repository-file-5c1aeb.s3.amazonaws.com (github-production-repository-file-5c1aeb.s3.amazonaws.com)|52.216.100.187|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7094966 (6.8M) [application/gzip]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                     0%[                    ]       0  --.-KB/s               common_voice_it_19574386.wav\n",
      "-                     0%[                    ]  59.16K   162KB/s               common_voice_it_19574384.wav\n",
      "-                     3%[                    ] 212.16K   291KB/s               common_voice_it_19574474.wav\n",
      "common_voice_it_19574387.wav\n",
      "common_voice_it_19574476.wav\n",
      "common_voice_it_19574477.wav\n",
      "common_voice_it_19574478.wav\n",
      "common_voice_it_19574498.wav\n",
      "-                    13%[=>                  ] 926.16K   846KB/s               common_voice_it_19574501.wav\n",
      "dev.csv\n",
      "common_voice_it_20030673.wav\n",
      "common_voice_it_20030672.wav\n",
      "common_voice_it_20030676.wav\n",
      "common_voice_it_20030681.wav\n",
      "common_voice_it_20030712.wav\n",
      "common_voice_it_20030713.wav\n",
      "common_voice_it_20030714.wav\n",
      "common_voice_it_20030715.wav\n",
      "common_voice_it_20030716.wav\n",
      "test.csv\n",
      "other.csv\n",
      "common_voice_it_19574500.wav\n",
      "common_voice_it_19712603.wav\n",
      "-                    41%[=======>            ]   2.83M  2.19MB/s               common_voice_it_19712602.wav\n",
      "common_voice_it_19712604.wav\n",
      "common_voice_it_19712608.wav\n",
      "common_voice_it_19712685.wav\n",
      "common_voice_it_19712686.wav\n",
      "common_voice_it_19712687.wav\n",
      "common_voice_it_19712688.wav\n",
      "common_voice_it_19712717.wav\n",
      "common_voice_it_19712721.wav\n",
      "train.csv\n",
      "common_voice_it_19970594.wav\n",
      "common_voice_it_19970963.wav\n",
      "common_voice_it_19971031.wav\n",
      "common_voice_it_19971036.wav\n",
      "common_voice_it_19971056.wav\n",
      "common_voice_it_19971088.wav\n",
      "common_voice_it_19971147.wav\n",
      "common_voice_it_19971142.wav\n",
      "common_voice_it_19971152.wav\n",
      "-                    81%[===============>    ]   5.54M  3.70MB/s               validated.csv\n",
      "common_voice_it_20060953.wav\n",
      "common_voice_it_17544185.wav\n",
      "common_voice_it_20045040.wav\n",
      "common_voice_it_20042813.wav\n",
      "common_voice_it_20001185.wav\n",
      "common_voice_it_19997999.wav\n",
      "common_voice_it_20033266.wav\n",
      "common_voice_it_19973815.wav\n",
      "-                   100%[===================>]   6.77M  4.07MB/s    in 1.7s    \n",
      "\n",
      "2020-05-18 12:58:33 (4.07 MB/s) - written to stdout [7094966/7094966]\n",
      "\n",
      "common_voice_it_20059124.wav\n",
      "--2020-05-18 12:58:37--  https://github.com/MozillaItalia/DeepSpeech-Italian-Model/raw/master/DeepSpeech/italian_alphabet.txt\n",
      "Resolving github.com (github.com)... 140.82.114.3\n",
      "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/MozillaItalia/DeepSpeech-Italian-Model/master/DeepSpeech/italian_alphabet.txt [following]\n",
      "--2020-05-18 12:58:38--  https://raw.githubusercontent.com/MozillaItalia/DeepSpeech-Italian-Model/master/DeepSpeech/italian_alphabet.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 86 [text/plain]\n",
      "Saving to: ‘/mnt/models/alphabet.txt’\n",
      "\n",
      "/mnt/models/alphabe 100%[===================>]      86  --.-KB/s    in 0s      \n",
      "\n",
      "2020-05-18 12:58:39 (4.10 MB/s) - ‘/mnt/models/alphabet.txt’ saved [86/86]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#extract the tiny cv sample dataset\n",
    "!mkdir -p /mnt/extracted/data/cv-it_tiny\n",
    "%env CV_TINY_PATH /mnt/extracted/data/cv-it_tiny\n",
    "!wget -O - https://github.com/MozillaItalia/DeepSpeech-Italian-Model/files/4610711/cv-it_tiny.tar.gz | tar -zxv -C $CV_TINY_PATH\n",
    "\n",
    "#and the italian alphabet\n",
    "!mkdir -p /mnt/models\n",
    "!wget -O \"/mnt/models/alphabet.txt\" https://github.com/MozillaItalia/DeepSpeech-Italian-Model/raw/master/DeepSpeech/italian_alphabet.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UQ-pzO3O3cEt"
   },
   "source": [
    "## CV-IT and MAILABS complete dataset\n",
    "\n",
    "**WARNING: the remaining space disk will not be enough to save training checkpoints**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ladVQoVJ9drR"
   },
   "outputs": [],
   "source": [
    "# Uncomment here if you want the CV-IT and MAILABS complete datasets\n",
    "# Keep in mind that all these stuff decompressed takes around 30GB\n",
    "# Note: english compatibility is not handled right now\n",
    "\n",
    "#MAILABS\n",
    "'''\n",
    "\n",
    "%cd $H/ds\n",
    "# Download and prepare M-AILABS\n",
    "!python bin/import_m-ailabs.py ${IMPORT_AS_ENGLISH} \\\n",
    "  --filter_alphabet /mnt/models/alphabet.txt \\\n",
    "  --language it_IT                           \\\n",
    "  /mnt/extracted/data/M-AILABS/\n",
    "# free some space removing the MAILABS tgz\n",
    "!rm /mnt/extracted/data/M-AILABS/it_IT.tgz\n",
    "\n",
    "'''\n",
    "\n",
    "# CV-IT\n",
    "'''\n",
    "%cd $H/ds\n",
    "# Download CV\n",
    "!mkdir -p /mnt/sources\n",
    "!wget https://voice-prod-bundler-ee1969a6ce8178826482b88e843c335139bd3fb4.s3.amazonaws.com/cv-corpus-4-2019-12-10/it.tar.gz -O /mnt/sources/it.tar.gz\n",
    "# Prepare CV\n",
    "!mkdir -p /mnt/extracted/data/cv-it/\n",
    "!tar -C /mnt/extracted/data/cv-it/ -xf /mnt/sources/it.tar.gz\n",
    "!python bin/import_cv2.py --filter_alphabet=/mnt/models/alphabet.txt /mnt/extracted/data/cv-it/\n",
    "# free some space again\n",
    "!rm /mnt/sources/it.tar.gz\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "O79LJ8UxrxQq",
    "outputId": "b65c63cd-e82c-4b6d-df91-9b01b9d4a85a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 1.5.0+cu101\n",
      "Uninstalling torch-1.5.0+cu101:\n",
      "  Successfully uninstalled torch-1.5.0+cu101\n"
     ]
    }
   ],
   "source": [
    "# Run this if you need some space\n",
    "!rm -rf /swift/*\n",
    "!pip uninstall -y torch\n",
    "!rm -rf $H/.cache/pip/*\n",
    "!rm -rf $H/DeepSpeech-Italian-Model/.git/*\n",
    "!rm -rf $H/ds/.git/*\n",
    "!rm -rf $H/kenlm/.git/*\n",
    "!rm -rf /content/sample_data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_vmJT9fZfHv9"
   },
   "source": [
    "## **Setup your google drive now!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-axrZPOUr9f"
   },
   "source": [
    "Please, before running other cells, export your google drive path to store your model checkpoints.\n",
    "\n",
    "Probably you'll need more space than colab offers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "30FPr_a97TD1",
    "outputId": "3748d20e-2e49-4d5a-fcc5-91323aa2be08"
   },
   "outputs": [],
   "source": [
    "# Save your google drive path\n",
    "%env GDRIVE_PATH /your/path/to/drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FXM5KLfAvzv"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nqoH_ho9VY_V"
   },
   "source": [
    "### Download DeepSpeech checkpoints\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "14L-a3abd659",
    "outputId": "1628e554-b2ae-4b49-b584-d7854d482ce7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2020-05-15 18:17:16--  https://github.com/mozilla/DeepSpeech/releases/download/v0.7.1/deepspeech-0.7.1-checkpoint.tar.gz\n",
      "Resolving github.com (github.com)... 140.82.113.4\n",
      "Connecting to github.com (github.com)|140.82.113.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/3682a100-9470-11ea-9ca7-6abad389183f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200515T181716Z&X-Amz-Expires=300&X-Amz-Signature=f8b00e4c3776c2527fe9ec58542ec29b101b1ab8e6555e2e0164ac40b1ed6710&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.7.1-checkpoint.tar.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2020-05-15 18:17:16--  https://github-production-release-asset-2e65be.s3.amazonaws.com/60273704/3682a100-9470-11ea-9ca7-6abad389183f?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20200515%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20200515T181716Z&X-Amz-Expires=300&X-Amz-Signature=f8b00e4c3776c2527fe9ec58542ec29b101b1ab8e6555e2e0164ac40b1ed6710&X-Amz-SignedHeaders=host&actor_id=0&repo_id=60273704&response-content-disposition=attachment%3B%20filename%3Ddeepspeech-0.7.1-checkpoint.tar.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)... 52.217.16.212\n",
      "Connecting to github-production-release-asset-2e65be.s3.amazonaws.com (github-production-release-asset-2e65be.s3.amazonaws.com)|52.217.16.212|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 644570384 (615M) [application/octet-stream]\n",
      "Saving to: ‘STDOUT’\n",
      "\n",
      "-                     0%[                    ]       0  --.-KB/s               ._deepspeech-0.7.1-checkpoint\n",
      "deepspeech-0.7.1-checkpoint/\n",
      "deepspeech-0.7.1-checkpoint/._checkpoint\n",
      "deepspeech-0.7.1-checkpoint/checkpoint\n",
      "deepspeech-0.7.1-checkpoint/._best_dev-732522.data-00000-of-00001\n",
      "deepspeech-0.7.1-checkpoint/best_dev-732522.data-00000-of-00001\n",
      "-                   100%[===================>] 614.71M  34.0MB/s    in 21s     \n",
      "\n",
      "2020-05-15 18:17:37 (30.0 MB/s) - written to stdout [644570384/644570384]\n",
      "\n",
      "deepspeech-0.7.1-checkpoint/._best_dev-732522.index\n",
      "deepspeech-0.7.1-checkpoint/best_dev-732522.index\n",
      "deepspeech-0.7.1-checkpoint/._best_dev_checkpoint\n",
      "deepspeech-0.7.1-checkpoint/best_dev_checkpoint\n",
      "deepspeech-0.7.1-checkpoint/._best_dev-732522.meta\n",
      "deepspeech-0.7.1-checkpoint/best_dev-732522.meta\n",
      "deepspeech-0.7.1-checkpoint/._alphabet.txt\n",
      "deepspeech-0.7.1-checkpoint/alphabet.txt\n"
     ]
    }
   ],
   "source": [
    "!wget -O - https://github.com/mozilla/DeepSpeech/releases/download/v0.7.1/deepspeech-0.7.1-checkpoint.tar.gz | tar -zxv -C \"$GDRIVE_PATH/0.7/transfer_ckpts/eng\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V7e62swUV02y"
   },
   "source": [
    "### Setup all needed params and paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7GPiNTuPAkPs",
    "outputId": "ece64cc0-e611-40be-b0b4-f7343402d595"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do early stop\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# lets put some model params\n",
    "\n",
    "BATCH_SIZE=2 #64 default\n",
    "N_HIDDEN=2048\n",
    "EPOCHS=30\n",
    "LEARNING_RATE=0.0001\n",
    "DROPOUT=0.3\n",
    "LM_ALPHA=0.65\n",
    "LM_BETA=1.45\n",
    "BEAM_WIDTH=500\n",
    "do_early_stop= True\n",
    "use_amp= False # DS checkpoint are not compatible with AMP\n",
    "\n",
    "#transfer params\n",
    "DROP_SOURCE_LAYERS = 1\n",
    "\n",
    "#other\n",
    "MAX_TO_KEEP = 3\n",
    "\n",
    "EARLY_STOP_FLAG=\"--noearly_stop\"\n",
    "if do_early_stop:\n",
    "  print(\"do early stop\")\n",
    "  EARLY_STOP_FLAG=\"--early_stop\"\n",
    "\n",
    "AMP_FLAG=\"\"\n",
    "if use_amp:\n",
    "  print(\"use automatic mixed precision\")\n",
    "  AMP_FLAG=\"--automatic_mixed_precision true\"\n",
    "\n",
    "!mkdir -p /mnt/sources/feature_cache || true\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbnvvYK6Ow9V"
   },
   "outputs": [],
   "source": [
    "#Some paths\n",
    "ALPHABET_CONFIG_PATH=\"/mnt/models/alphabet.txt\"\n",
    "\n",
    "SAVE_CHECKPOINT_DIR = os.path.join(os.environ.get(\"GDRIVE_PATH\"),\"0.7/transfer_ckpts/ita\")\n",
    "LOAD_CHECKPOINT_DIR = os.path.join(os.environ.get(\"GDRIVE_PATH\"),\"0.7/transfer_ckpts/eng/deepspeech-0.7.1-checkpoint\")\n",
    "\n",
    "# when you set 2 different dir during training deepspeech will warn you that it will\n",
    "# be impossibile to evaluate the model with the test dataset.\n",
    "CHECKPOINT_DIR = SAVE_CHECKPOINT_DIR\n",
    "\n",
    "SCORER = os.path.join(os.environ.get(\"GDRIVE_PATH\"),\"0.7/kenlm.scorer\")\n",
    "\n",
    "EXPORT_FOLDER = os.path.join(os.environ.get(\"GDRIVE_PATH\"),\"0.7\")\n",
    "\n",
    "#dir for tensorboard and logs\n",
    "SUMMARY_DIR = os.path.join(os.environ.get(\"GDRIVE_PATH\"),\"0.7/logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkFqEJWhMhJV"
   },
   "outputs": [],
   "source": [
    "# Train, dev and test as list of path to .csv files\n",
    "# Note: each dataset needs to be located under /mnt/extracted/data\n",
    "\n",
    "all_train_csv=!(find /mnt/extracted/data/ -type f -name '*train.csv' -printf '%p,' | sed -e 's/,$//g')\n",
    "all_dev_csv=!(find /mnt/extracted/data/ -type f -name '*dev.csv' -printf '%p,' | sed -e 's/,$//g')\n",
    "all_test_csv=!(find /mnt/extracted/data/ -type f -name '*test.csv' -printf '%p,' | sed -e 's/,$//g')\n",
    "ALL_TRAIN_CSV=all_train_csv[0]\n",
    "ALL_DEV_CSV=all_dev_csv[0]\n",
    "ALL_TEST_CSV=all_test_csv[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "-wlOBFwzSUc5",
    "outputId": "3c05ba41-3db6-4dbd-bfc3-c990ef75ab1a"
   },
   "outputs": [],
   "source": [
    "# Build the params string for DeepSpeech.py\n",
    "\n",
    "'''\n",
    "Note if your paths contains spaces just wrap them inside \" \"\n",
    "eg:\n",
    "\n",
    "--scorer \"'+SCORER+'\" \\\n",
    "'''\n",
    "\n",
    "params = \"\"\n",
    "train_params = ' \\\n",
    "--summary_dir \"'+SUMMARY_DIR+'\" \\\n",
    "--log_dir \"'+SUMMARY_DIR+'\" \\\n",
    "--alphabet_config_path '+ALPHABET_CONFIG_PATH+' \\\n",
    "--checkpoint_dir \"'+CHECKPOINT_DIR+'\" \\\n",
    "--show_progressbar true \\\n",
    "--train_cudnn True \\\n",
    "'+AMP_FLAG+' \\\n",
    "--scorer \"'+SCORER+'\" \\\n",
    "--train_files '+ALL_TRAIN_CSV+' \\\n",
    "--dev_files '+ALL_DEV_CSV+' \\\n",
    "--train_batch_size '+str(BATCH_SIZE)+' \\\n",
    "--dev_batch_size '+str(BATCH_SIZE)+' \\\n",
    "--n_hidden '+str(N_HIDDEN)+' \\\n",
    "--epochs '+str(EPOCHS)+' \\\n",
    "--learning_rate '+str(LEARNING_RATE)+' \\\n",
    "--dropout_rate '+str(DROPOUT)+' \\\n",
    "--lm_alpha '+str(LM_ALPHA)+' \\\n",
    "--lm_beta '+str(LM_BETA)+' \\\n",
    "--max_to_keep '+str(MAX_TO_KEEP)+' \\\n",
    "'+EARLY_STOP_FLAG+' \\\n",
    "--verbosity 1'\n",
    "\n",
    "params = train_params\n",
    "\n",
    "# If you dont want to use data augmentation, flag this on False\n",
    "use_augmentation = True\n",
    "# transfer learning on/off\n",
    "do_transfer_learning = True\n",
    "\n",
    "\n",
    "if do_transfer_learning:\n",
    "  transfer_params = '\\\n",
    "  --drop_source_layers '+str(DROP_SOURCE_LAYERS)+'\\\n",
    "  --save_checkpoint_dir \"'+SAVE_CHECKPOINT_DIR+'\" \\\n",
    "  --load_checkpoint_dir \"'+LOAD_CHECKPOINT_DIR+'\"'\n",
    "\n",
    "  params+=transfer_params\n",
    "\n",
    "if use_augmentation:\n",
    "  augm = '\\\n",
    "  --augmentation_freq_and_time_masking True \\\n",
    "  --augmentation_freq_and_time_masking_freq_mask_range 5 \\\n",
    "  --augmentation_freq_and_time_masking_number_freq_masks 3 \\\n",
    "  --augmentation_freq_and_time_masking_time_mask_range 2 \\\n",
    "  --augmentation_freq_and_time_masking_number_time_masks 3 \\\n",
    "  --augmentation_pitch_and_tempo_scaling True \\\n",
    "  --augmentation_pitch_and_tempo_scaling_min_pitch 0.95 \\\n",
    "  --augmentation_pitch_and_tempo_scaling_max_pitch 1.2 \\\n",
    "  --augmentation_pitch_and_tempo_scaling_max_tempo 1.2 \\\n",
    "  --augmentation_speed_up_std 0.1 \\\n",
    "  --data_aug_features_multiplicative 1.0 \\\n",
    "  --data_aug_features_additive 1.0'\n",
    "  params+=augm\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28CMI8f7WJHX"
   },
   "source": [
    "### Start the training phase!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "aoiWWxQlGSC6",
    "outputId": "268ecd06-999b-4c44-92df-17e5675ce25b"
   },
   "outputs": [],
   "source": [
    "%cd $H/ds/\n",
    "!set -xe\n",
    "!python DeepSpeech.py $params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeMf4QiUWnJl"
   },
   "source": [
    "### Evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "h7KomzFFp4aV",
    "outputId": "97726475-70bf-44a2-dfcc-0beedf00053b"
   },
   "outputs": [],
   "source": [
    "%cd $H/ds/\n",
    "# After trainining/transfer/finetuning do some test here\n",
    "# TODO: SOO SLOW!! Does it evaluate on cpu?? Is WER calc that slows it down?\n",
    "test_params = ' \\\n",
    "--alphabet_config_path '+ALPHABET_CONFIG_PATH+' \\\n",
    "--checkpoint_dir \"'+CHECKPOINT_DIR+'\" \\\n",
    "--show_progressbar true \\\n",
    "--load_evaluate best \\\n",
    "--scorer \"'+SCORER+'\" \\\n",
    "--train_cudnn True \\\n",
    "--test_files '+ALL_TEST_CSV+' \\\n",
    "--test_batch_size '+str(BATCH_SIZE)+' \\\n",
    "--verbosity 2'\n",
    "\n",
    "!python DeepSpeech.py $test_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oLbRRiS1Wzzi"
   },
   "source": [
    "### Export the .pb file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309
    },
    "colab_type": "code",
    "id": "0-FuqgNIKago",
    "outputId": "a3cdf3d7-cb1e-4ceb-af7a-4b035ed24e84"
   },
   "outputs": [],
   "source": [
    "%cd $H/ds/\n",
    "#export .pb file\n",
    "exp_params = ' \\\n",
    "--alphabet_config_path '+ALPHABET_CONFIG_PATH+' \\\n",
    "--checkpoint_dir \"'+CHECKPOINT_DIR+'\" \\\n",
    "--show_progressbar true \\\n",
    "--load_evaluate \"best\" \\\n",
    "--scorer \"'+SCORER+'\" \\\n",
    "--lm_alpha '+str(LM_ALPHA)+' \\\n",
    "--lm_beta '+str(LM_BETA)+' \\\n",
    "--export_dir \"'+EXPORT_FOLDER+'\" \\\n",
    "--export_language \"it\" \\\n",
    "--verbosity 1'\n",
    "\n",
    "!python -u DeepSpeech.py $exp_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8IfKjgVvXD5l"
   },
   "source": [
    "### Create the pbmm file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "zuTux9ErMc6L",
    "outputId": "ec9dcd85-df30-4098-cea1-54b72bf2b824"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://community-tc.services.mozilla.com/api/index/v1/task/project.deepspeech.tensorflow.pip.r1.15.cpu/artifacts/public/convert_graphdef_memmapped_format ...\n",
      "Downloading: 100%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#lets create the pbmm format\n",
    "!python util/taskcluster.py --source tensorflow --artifact convert_graphdef_memmapped_format --branch r1.15 --target .\n",
    "!./convert_graphdef_memmapped_format --in_graph=\"$EXPORT_FOLDER\"/output_graph.pb --out_graph=\"$EXPORT_FOLDER\"/output_graph.pbmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6atYCxVLfO8f"
   },
   "outputs": [],
   "source": [
    "!python DeepSpeech.py --helpfull"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "deepspeech_training.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
